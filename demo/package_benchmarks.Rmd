---
title: "Performance Benchmark for a Single Dyad" 
author: "Themis N. Efthimiou" 
date: "`r Sys.Date()`" 
output:
  html_document:
    toc: true
    df_print: paged
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
---

```{r}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 7,
  dpi = 300
)
```

# Introduction

This notebook provides a performance benchmark for the `duet` R package using a single, real-world dyadic dataset from a five-minute recording. The analysis focuses on understanding the computational time and memory usage of the core functions in a typical processing pipeline.

## 1\. Environment Setup

This section loads the necessary libraries for the analysis. It will automatically install any missing packages.

```{r load-libraries, message=FALSE}
# Essential libraries for benchmarking and data manipulation
required_packages <- c(
  "remotes", "bench", "ggplot2", "dplyr", "tidyr",
  "knitr", "readr", "profmem"
)

# Install any packages that are not already present
missing_packages <- required_packages[!(required_packages %in% installed.packages()[,"Package"])]
if(length(missing_packages)) {
  install.packages(missing_packages, dependencies = TRUE)
}

# Load all required libraries
suppressPackageStartupMessages({
  lapply(required_packages, library, character.only = TRUE)
})

# Install the duet package from GitHub if not already installed
if (!"duet" %in% installed.packages()[,"Package"]) {
  remotes::install_github("ThemisEfth/duet", upgrade = "never")
}
library(duet)

# --- Parameters ---
# Rationale: Setting the video's frames per second (FPS) is crucial for accurate
# velocity calculation. 30 FPS is a common standard for such recordings.
VIDEO_FPS <- 30
```

## 2\. Data Loading

Here, we load the specific dyadic data from the provided CSV file.

```{r data-load}
# Define the path to the data file
# Rationale: Placing the file path in a variable makes it easy to update if needed.
csv_file_path <- './openpose_csv/Dyad_10/Dyad-10.0_B_IDs-B013-B016_body_dyad.csv'

# Check if the file exists before attempting to read it
# Rationale: This is a defensive programming practice that provides a clear
# error message if the data is not found, rather than letting the script fail ambiguously.
if (!file.exists(csv_file_path)) {
  stop("Data file not found at the specified path: ", csv_file_path)
}

# Read the data into a dataframe
df_raw <- read_csv(csv_file_path)

# Display the first few rows to confirm it's loaded correctly
cat("Data loaded successfully.\n")
cat("Number of rows:", nrow(df_raw), "\n")
knitr::kable(head(df_raw))
```

## 3\. Pipeline Performance Benchmark

This section benchmarks each major data processing step in the `duet` package. Each function is timed, and its memory allocation is measured. The output of each step is used as the input for the next, simulating a real-world workflow.

```{r benchmark-pipeline}
# Run the full pipeline and benchmark each step
# Rationale: Using bench::mark provides a robust way to measure performance,
# running each expression multiple times to get a reliable median execution time
# and garbage collection statistics.
benchmark_results <- bench::mark(
  `1_Keypoint_Labeling` = {
    df_labeled <- op_apply_keypoint_labels(df_raw)
  },
  `2_Interpolation` = {
    df_interpolated <- op_interpolate(df_labeled, confidence_threshold = 0.3)
  },
  `3_Velocity_Calculation` = {
    df_velocity <- op_compute_velocity(df_interpolated, merge_xy = TRUE, fps = VIDEO_FPS)
  },
  `4_Motion_Energy` = {
    df_motion_energy <- op_compute_motionenergy(df_raw, plot = FALSE)
  },
  `5_Coherence_Analysis` = {
    df_coherence <- op_compute_coherence(df_motion_energy, plot = FALSE)
  },
  iterations = 3,  # Fewer iterations because some steps can be slow
  check = FALSE    # We manually pass objects, so we don't need to check for output equality
)

# Display the benchmark summary table
print("=== PIPELINE BENCHMARK RESULTS ===")
print(benchmark_results[, c("expression", "min", "median", "itr/sec", "mem_alloc", "n_gc")])
```

## 4\. Memory Profiling

While `bench::mark` gives us the total memory allocated during an expression, `profmem` can provide a more detailed look at which objects are being created. We will profile the entire pipeline to understand the cumulative memory footprint.

```{r memory-profile}
# Profile the memory usage of the entire pipeline
# Rationale: This gives a holistic view of the memory pressure exerted by a full
# analysis run, which is more informative than profiling functions in isolation.
memory_profile <- profmem({
  df_labeled <- op_apply_keypoint_labels(df_raw)
  df_interpolated <- op_interpolate(df_labeled, confidence_threshold = 0.3)
  df_velocity <- op_compute_velocity(df_interpolated, merge_xy = TRUE, fps = VIDEO_FPS)
  df_motion_energy <- op_compute_motionenergy(df_raw, plot = FALSE)
  df_coherence <- op_compute_coherence(df_motion_energy, plot = FALSE)
})

# Print the total memory allocated during the process
cat("=== MEMORY PROFILING SUMMARY ===\n")
cat("Total memory allocated during the full pipeline execution:\n")
print(total(memory_profile))
```

## 5\. Summary and Report

This final section provides a clear summary of the performance benchmarks.

### Benchmark Timings

The following plot visualises the median time taken for each step in the processing pipeline.

```{r plot-results}
# Plot the results for a clear visual comparison
# Rationale: A visual plot makes it much easier to quickly identify the most
# computationally expensive steps in the pipeline.
plot <- ggplot(benchmark_results, aes(x = expression, y = median, fill = expression)) +
  geom_col() +
  labs(
    title = "Duet Package: Median Execution Time per Pipeline Step",
    subtitle = paste("Based on", nrow(df_raw), "rows of data"),
    x = "Pipeline Step",
    y = "Median Time (seconds)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, hjust = 1)) +
  # Use the actual median time values as labels on the bars
  geom_text(aes(label = bench::time_format(median)), vjust = -0.5, size = 4)

print(plot)

# Save the benchmark results to a file
# Rationale: Saving the raw benchmark object allows for later, more in-depth
# analysis without needing to re-run the computationally expensive benchmarks.
if (!dir.exists("benchmark_results")) {
  dir.create("benchmark_results")
}
saveRDS(
  list(
    benchmark_summary = benchmark_results,
    memory_profile = memory_profile
  ),
  file = "./benchmark_results/single_dyad_benchmark.rds"
)

cat("\n=== BENCHMARKING COMPLETE ===\n")
cat("Benchmark results object saved to: ./benchmark_results/single_dyad_benchmark.rds\n")
```
